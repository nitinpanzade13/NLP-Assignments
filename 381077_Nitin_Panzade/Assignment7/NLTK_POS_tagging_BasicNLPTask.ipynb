{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb24102",
   "metadata": {},
   "source": [
    "## **1. Environment Setup**\n",
    "#### First, we need to install NLTK and download the necessary datasets (corpora) for tokenization and tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b368e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nitin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\nitin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\nitin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nitin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nitin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nitin\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nitin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nitin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\nitin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nitin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nitin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\nitin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nitin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install NLTK\n",
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "# Download essential NLTK resources\n",
    "nltk.download('punkt')         # For tokenization\n",
    "nltk.download('averaged_perceptron_tagger') # For POS tagging\n",
    "nltk.download('averaged_perceptron_tagger_eng') # For English POS tagging\n",
    "nltk.download('stopwords')     # For removing common words\n",
    "nltk.download('wordnet')       # For Lemmatization\n",
    "nltk.download('omw-1.4') # For WordNet data, including synonyms and antonyms\n",
    "nltk.download('punkt_tab') # For tokenization of tab-separated text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6ef6f1",
   "metadata": {},
   "source": [
    "## **2. Text Preprocessing Functions**\n",
    "#### Preprocessing is the \"cleaning\" phase. It transforms raw text into a format that algorithms can actually understand.\n",
    "\n",
    "### Step A: Tokenization & Noise Removal\n",
    "#### We convert the text to lowercase, remove punctuation, and split it into individual words (tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab681f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Tokens: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', \"'s\", 'sunny', 'day', 'neighborhood']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 3. Remove Punctuation and Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_tokens = [w for w in tokens if w not in stop_words and w not in string.punctuation]\n",
    "    \n",
    "    return cleaned_tokens\n",
    "\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog! It's a sunny day in the neighborhood.\"\n",
    "tokens = preprocess_text(sample_text)\n",
    "print(f\"Cleaned Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba4e949",
   "metadata": {},
   "source": [
    "## **3. POS Tagging (Part-of-Speech)**\n",
    "#### POS tagging assigns a category to each word (Noun, Verb, Adjective, etc.) based on its definition and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd2539a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NNS'), ('lazy', 'VBP'), ('dog', 'NN'), (\"'s\", 'POS'), ('sunny', 'JJ'), ('day', 'NN'), ('neighborhood', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "def get_pos_tags(tokens):\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "pos_tags = get_pos_tags(tokens)\n",
    "print(\"POS Tags:\", pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d4691",
   "metadata": {},
   "source": [
    "##### Note: NLTK uses the Penn Treebank Tagset. For example, NN stands for Noun, and JJ stands for Adjective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a3ab88",
   "metadata": {},
   "source": [
    "## **4. Lemmatization**\n",
    "#### Unlike Stemming (which chops off ends of words), Lemmatization uses a vocabulary and morphological analysis to return the word to its dictionary base form (the lemma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "119b7ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized: ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog', \"'s\", 'sunny', 'day', 'neighborhood']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "lemmas = lemmatize_tokens(tokens)\n",
    "print(\"Lemmatized:\", lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abda434d",
   "metadata": {},
   "source": [
    "## **5. Frequency Analysis Application**\n",
    "#### Let's wrap this into a simple analysis tool that identifies the most frequent meaningful words in a text block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc3934b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Words Analysis:\n",
      "            Word  Frequency\n",
      "1   intelligence          3\n",
      "0     artificial          2\n",
      "13          data          2\n",
      "10        system          2\n",
      "4         allows          1\n",
      "5        machine          1\n",
      "2   transforming          1\n",
      "3          world          1\n",
      "7     experience          1\n",
      "6          learn          1\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "def analyze_text(text):\n",
    "    # Process\n",
    "    tokens = preprocess_text(text)\n",
    "    lemmas = lemmatize_tokens(tokens)\n",
    "    \n",
    "    # Frequency Distribution\n",
    "    fdist = FreqDist(lemmas)\n",
    "    \n",
    "    # Convert to Dataframe for clean display\n",
    "    df_freq = pd.DataFrame(fdist.items(), columns=['Word', 'Frequency']).sort_values(by='Frequency', ascending=False)\n",
    "    \n",
    "    return df_freq.head(10)\n",
    "\n",
    "# Example Usage\n",
    "blog_post = \"\"\"\n",
    "Artificial intelligence is transforming the world. Intelligence allows machines \n",
    "to learn from experience. Learning is essential for artificial systems to improve.\n",
    "Most intelligence systems rely on data. Data is the new oil.\n",
    "\"\"\"\n",
    "\n",
    "result = analyze_text(blog_post)\n",
    "print(\"Top 10 Words Analysis:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd630f7",
   "metadata": {},
   "source": [
    "## **Summary of Workflow**\n",
    " **1. Normalization:** Converting text to lowercase.\n",
    "\n",
    " **2. Tokenization:** Breaking sentences into word units.\n",
    "\n",
    " **3. Stopword Removal:** Eliminating \"fluff\" words (the, is, at).\n",
    "\n",
    " **4. POS Tagging:** Identifying the grammatical role of words.\n",
    " \n",
    " **5. Lemmatization:** Reducing words to their root form for consistent counting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
